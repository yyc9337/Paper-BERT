## BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding


### Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova
### Google AI Language

### Abstract

BERT 는 새로운 언어 표현 모델입니다. 기존의 representation models(Peters et al., 2018a; Radford et al., 2018)과는 다르게 BERT는 모든 계층의 왼쪽 및 오른쪽 컨텍스트에서 공동으로 조건화함으로써 레이블이 없는 텍스트에서 심층 양방향 표현을 사전 학습하도록 설계되었다. BERT는 한개의 추가 출력 계층으로 미세하게 조정되어 작업 아키텍쳐 수정 없이 답장, 추론과 같은 작업에 대한 최신 모델을 생성한다.
BERT는 강력한 성능을 자랑하는데, 11개의 자연어 처리에서 최고의 결과를 얻을 수 있다. GLUE 점수를 80.5%, MultiNLI 정확도를 86.7%로 높이는 작업을 포함한다. SQuAD v1.1 문항은 93.2로, SQuAD v2.0 시험 F1은 83.1이다.
